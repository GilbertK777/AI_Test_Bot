"""
XGBoost ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì „ì²´ ìƒëª…ì£¼ê¸°(í•™ìŠµ, ì˜ˆì¸¡, ì €ì¥/ë¡œë“œ)ë¥¼ ê´€ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤.

ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1.  **ëª¨ë¸ ë¡œë”©**: ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ, ì§€ì •ëœ ê²½ë¡œì— ì €ì¥ëœ ê¸°ì¡´ ëª¨ë¸ íŒŒì¼(.joblib)ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
2.  **ëª¨ë¸ í•™ìŠµ (`train`)**:
    -   **ë°ì´í„° ì¤€ë¹„**: ì…ë ¥ëœ ë°ì´í„°í”„ë ˆì„ì—ì„œ í•™ìŠµì— ì‚¬ìš©í•  í”¼ì²˜(X)ì™€ íƒ€ê²Ÿ(y)ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    -   **ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬**: `SMOTE` (Synthetic Minority Over-sampling Technique)ë¥¼ ì‚¬ìš©í•˜ì—¬
        ìƒìŠ¹(1)ê³¼ í•˜ë½(0) íƒ€ê²Ÿì˜ ë¹„ìœ¨ì„ ì¸ìœ„ì ìœ¼ë¡œ ë§ì¶° ëª¨ë¸ì´ í•œìª½ìœ¼ë¡œ í¸í–¥ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    -   **ì£¼ê¸°ì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: `CFG.GRID_DAYS` ì£¼ê¸°ë¡œ `GridSearchCV`ë¥¼ ì‹¤í–‰í•˜ì—¬
        ìµœì ì˜ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°(`n_estimators`, `max_depth` ë“±)ë¥¼ íƒìƒ‰í•˜ê³ , ìµœì  ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
    -   **ì ì§„ì  í•™ìŠµ**: GridSearchCVë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠëŠ” ì£¼ê¸°ì—ëŠ”, ê¸°ì¡´ ëª¨ë¸ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬
        `n_estimators`ë¥¼ ëŠ˜ë ¤ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì¬í•™ìŠµ(online learning)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    -   **ëª¨ë¸ ì €ì¥**: í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì€ `joblib`ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
3.  **ì˜ˆì¸¡ (`add_prob`)**: í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì˜ ê° í–‰(ìº”ë“¤)ì— ëŒ€í•´
    'ë‹¤ìŒ ìº”ë“¤ ê°€ê²©ì´ ìƒìŠ¹í•  í™•ë¥ ' (`prob_up`)ì„ ì˜ˆì¸¡í•˜ê³ , ì´ ê°’ì„ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
"""
import logging
import joblib
from datetime import datetime
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV
from config.config import CFG
from src.utils.helpers import tg

class ModelService:
    """XGBoost ëª¨ë¸ ê´€ë¦¬ (í•™ìŠµ/ì˜ˆì¸¡/ì €ì¥/ë¡œë“œ) í´ë˜ìŠ¤"""

    # ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë  í”¼ì²˜(íŠ¹ì„±) ëª©ë¡.
    # `IndicatorRepository`ì—ì„œ ìƒì„±ëœ ë©€í‹°-íƒ€ì„í”„ë ˆì„ ì§€í‘œë“¤ì´ í¬í•¨ë©ë‹ˆë‹¤.
    FEATURES = ["close", "ema_fast", "ema_slow", "rsi",
                "rsi_1h", "ema_fast_4h", "ema_slow_4h",
                "atr", "macd", "macd_sig"]

    def __init__(self, path):
        """
        ModelService ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            path (Path): ëª¨ë¸ì„ ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¬ íŒŒì¼ ê²½ë¡œ ê°ì²´.
        """
        self.path = path
        # ì§€ì •ëœ ê²½ë¡œì— ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ `joblib.load`ë¥¼ í†µí•´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ `self.model`ì€ Noneìœ¼ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.
        self.model = joblib.load(path) if path.exists() else None
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œ ì‹œê°„ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë³€ìˆ˜. `datetime.min`ìœ¼ë¡œ ì´ˆê¸°í™”.
        self.t_last_train = datetime.min
        # ë§ˆì§€ë§‰ìœ¼ë¡œ GridSearchCVë¥¼ ì‹¤í–‰í•œ ì‹œê°„ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë³€ìˆ˜.
        self.t_last_grid = datetime.min

    def train(self, df):
        """
        ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ XGBoost ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

        í•™ìŠµ ê³¼ì •ì€ ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë‚˜ë‰©ë‹ˆë‹¤:
        1.  **ì „ì²´ í•™ìŠµ (GridSearch)**: ëª¨ë¸ì´ ì—†ê±°ë‚˜, ë§ˆì§€ë§‰ GridSearch ì´í›„ ì„¤ì •ëœ ê¸°ê°„(`CFG.GRID_DAYS`)ì´
            ì§€ë‚¬ì„ ê²½ìš° ìˆ˜í–‰ë©ë‹ˆë‹¤. ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
        2.  **ì ì§„ì  í•™ìŠµ**: ê¸°ì¡´ ëª¨ë¸ì´ ìˆê³  GridSearch ì£¼ê¸°ê°€ ì•„ë‹ ê²½ìš°, ê¸°ì¡´ ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬
            `n_estimators` (íŠ¸ë¦¬ì˜ ê°œìˆ˜)ë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì¬í•™ìŠµí•©ë‹ˆë‹¤.

        Args:
            df (pd.DataFrame): `IndicatorRepository`ì—ì„œ ìƒì„±ëœ, í”¼ì²˜ì™€ 'target' ì»¬ëŸ¼ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„.
        """
        logging.info("Starting model training...")
        # ë°ì´í„°í”„ë ˆì„ì—ì„œ í”¼ì²˜(X)ì™€ íƒ€ê²Ÿ(y)ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        X, y = df[self.FEATURES], df["target"]

        # ë°ì´í„°ë¥¼ 80%ì˜ í•™ìŠµìš©(train)ê³¼ 20%ì˜ ê²€ì¦ìš©(validation)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. (í˜„ì¬ ì½”ë“œì—ì„œëŠ” ê²€ì¦ìš©ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        split = int(len(df) * 0.8)
        X_train_orig, y_train_orig = X[:split], y[:split]

        # SMOTEë¥¼ ì‚¬ìš©í•˜ê¸° ì „ì—, ê° í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        n_samples_class_0 = (y_train_orig == 0).sum()
        n_samples_class_1 = (y_train_orig == 1).sum()
        n_minority_samples = min(n_samples_class_0, n_samples_class_1)

        # SMOTEëŠ” ìµœì†Œí•œ k_neighbors + 1 ê°œì˜ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤.
        # ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ SMOTEë¥¼ ì ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì´ ê²½ìš°ì—” ê·¸ëƒ¥ ì›ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if n_minority_samples > 1:
            # k_neighborsëŠ” ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.
            # ê¸°ë³¸ê°’(5)ë³´ë‹¤ ìƒ˜í”Œ ìˆ˜ê°€ ì ìœ¼ë©´, ìƒ˜í”Œ ìˆ˜ - 1ì„ k_neighborsë¡œ ì„¤ì •í•˜ì—¬ ì—ëŸ¬ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            k_neighbors = min(5, n_minority_samples - 1)
            logging.info(f"Applying SMOTE with k_neighbors={k_neighbors}")
            X_tr, y_tr = SMOTE(random_state=42, k_neighbors=k_neighbors).fit_resample(X_train_orig, y_train_orig)
        else:
            logging.warning("Skipping SMOTE due to insufficient samples in the minority class.")
            X_tr, y_tr = X_train_orig, y_train_orig

        # GridSearchë¥¼ ìˆ˜í–‰í•´ì•¼ í•  ì¡°ê±´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        # (1) self.modelì´ None (ì¦‰, í•œë²ˆë„ í•™ìŠµëœ ì  ì—†ìŒ) ì´ê±°ë‚˜,
        # (2) ë§ˆì§€ë§‰ GridSearchë¥¼ ì‹¤í–‰í•œ ì§€ `CFG.GRID_DAYS`ì¼ ì´ìƒ ê²½ê³¼í•œ ê²½ìš°.
        if self.model is None or (datetime.utcnow() - self.t_last_grid).days >= CFG.GRID_DAYS:
            logging.info("Performing full training with GridSearchCV...")
            # ê¸°ë³¸ XGBClassifier ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤. ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ `subsample`ê³¼ `colsample_bytree`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            base = XGBClassifier(subsample=0.8, colsample_bytree=0.8,
                                 use_label_encoder=False, eval_metric="logloss")
            # íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
            param_grid = {
                "n_estimators": [120, 160],  # íŠ¸ë¦¬ì˜ ê°œìˆ˜
                "max_depth": [3, 4],         # íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
                "learning_rate": [0.05, 0.1] # í•™ìŠµë¥ 
            }
            # GridSearchCV ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. `cv=3`ì€ 3-fold êµì°¨ ê²€ì¦ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. `n_jobs=-1`ì€ ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
            grid = GridSearchCV(base, param_grid, cv=3, n_jobs=-1)
            grid.fit(X_tr, y_tr)

            # íƒìƒ‰ ê²°ê³¼ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ `self.model`ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
            self.model = grid.best_estimator_
            self.t_last_grid = datetime.utcnow()
            logging.info(f"GridSearch finished. Best parameters: {grid.best_params_}")
        else:
            # ì ì§„ì  í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            logging.info("Performing incremental training...")
            # ê¸°ì¡´ ëª¨ë¸ì˜ `n_estimators` ê°’ì„ ê°€ì ¸ì™€ 40ë§Œí¼ ëŠ˜ë¦½ë‹ˆë‹¤.
            n_old = self.model.get_params().get("n_estimators", 100) # ê¸°ë³¸ê°’ 100
            self.model.set_params(n_estimators=n_old + 40)
            # `xgb_model` íŒŒë¼ë¯¸í„°ì— ê¸°ì¡´ ë¶€ìŠ¤í„°(booster)ë¥¼ ì „ë‹¬í•˜ì—¬ í•™ìŠµì„ ì´ì–´ê°‘ë‹ˆë‹¤.
            self.model.fit(X_tr, y_tr, xgb_model=self.model.get_booster())

        # í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        joblib.dump(self.model, self.path)
        self.t_last_train = datetime.utcnow()
        logging.info(f"Model training complete. Model saved to {self.path}")
        tg("ğŸ“ˆ ëª¨ë¸ ì¬í•™ìŠµ ì™„ë£Œ")

    def add_prob(self, df):
        """
        í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì— ìƒìŠ¹ í™•ë¥ (`prob_up`)ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            df (pd.DataFrame): í”¼ì²˜ ì»¬ëŸ¼ë“¤ì´ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„.

        Returns:
            pd.DataFrame: 'prob_up' ì»¬ëŸ¼ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„.
        """
        df = df.copy()
        if self.model:
            # `predict_proba`ëŠ” ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ ë°˜í™˜í•©ë‹ˆë‹¤. [P(class=0), P(class=1)]
            # `[:, 1]`ì„ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ 1(ìƒìŠ¹)ì— ëŒ€í•œ í™•ë¥ ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
            probabilities = self.model.predict_proba(df[self.FEATURES])
            df["prob_up"] = probabilities[:, 1]
        else:
            # ëª¨ë¸ì´ ì•„ì§ í•™ìŠµë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì¤‘ë¦½ì ì¸ ê°’ì¸ 0.5ë¡œ ì±„ì›ë‹ˆë‹¤.
            df["prob_up"] = 0.5
        return df
